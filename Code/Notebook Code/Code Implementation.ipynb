{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \n",
    "    # read data from files into list\n",
    "    \n",
    "    # create three sequences\n",
    "    index = []\n",
    "    hour = []\n",
    "    elevation = []\n",
    "    \n",
    "    # open file\n",
    "    infile = open(filename, \"r\")\n",
    "    \n",
    "    for line in infile:\n",
    "        numbers = line.split()\n",
    "        \n",
    "        if len(numbers) !=5:\n",
    "            continue\n",
    "            \n",
    "        cycle = float(numbers[0])\n",
    "        height = float(numbers[3])\n",
    "    \n",
    "        index.append(cycle)\n",
    "        hour.append((cycle - 1)/ 4)\n",
    "        elevation.append(height)\n",
    "    \n",
    "    return index, hour, elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_transform_csv(Index, Hour, Elevation, scale, number):    \n",
    "    \n",
    "    # transform list into csv    \n",
    "    \n",
    "    # slice three sequences according to timescale\n",
    "    column_1 = Index[scale:number:scale]\n",
    "    column_2 = Hour[scale:number:scale]\n",
    "    column_3 = Elevation[scale:number:scale]\n",
    "    \n",
    "    # define name of sequences\n",
    "    name =['Cycle', 'Hour', 'Elevation']\n",
    "    \n",
    "    # define csv\n",
    "    combine_list = []\n",
    "    \n",
    "    for i in range(len(column_1)):\n",
    "        combine_list.append([column_1[i], column_2[i], column_3[i]])\n",
    "    \n",
    "    data_csv = pd.DataFrame(columns = name, data = combine_list)\n",
    "\n",
    "    return data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onefigure_plot(start, end, x_axis, y_axis, colour, label_name, title_name, True_or_False):\n",
    "    \n",
    "    # plot one figure\n",
    "    \n",
    "    fig, ax1 = plt.subplots(1, figsize=(15, 5))\n",
    "    fig.tight_layout(w_pad=4)\n",
    "    \n",
    "    # draw a graph of one data  \n",
    "    ax1.plot(np.array(x_axis), np.array(y_axis), colour, label = label_name, markersize=5)\n",
    "\n",
    "    # set legend information\n",
    "    ax1.set_ylim([start, end]);\n",
    "    ax1.set_xlabel('Hours (h)', fontsize=16)\n",
    "    ax1.set_ylabel('Tidal elevation (m)', fontsize=16)\n",
    "    ax1.set_title(title_name, fontsize=16)\n",
    "    ax1.legend(loc='best', fontsize=14)\n",
    "    \n",
    "    # add grid or not\n",
    "    ax1.grid(True_or_False)\n",
    "    \n",
    "    return 'one figure plot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twofigure_plot(start, end, x_axis1, y_axis1, colour1, label_name1, x_axis2, y_axis2, colour2, label_name2, title_name, True_or_False):\n",
    "    \n",
    "    # plot two figures  \n",
    "    \n",
    "    fig, ax1 = plt.subplots(1, figsize=(15, 5))\n",
    "    fig.tight_layout(w_pad=4)\n",
    "    \n",
    "    # draw a graph of two kinds of data \n",
    "    ax1.plot(np.array(x_axis1), np.array(y_axis1), colour1, label = label_name1, markersize=5)\n",
    "    ax1.plot(np.array(x_axis2), np.array(y_axis2), colour2, label = label_name2, markersize=5)\n",
    "    \n",
    "    # set legend information\n",
    "    ax1.set_ylim([start, end]);\n",
    "    ax1.set_xlabel('Hours (h)', fontsize=16)\n",
    "    ax1.set_ylabel('Tidal elevation (m)', fontsize=16)\n",
    "    ax1.set_title(title_name, fontsize=16)\n",
    "    ax1.legend(loc='best', fontsize=14)\n",
    "\n",
    "    # add grid or not\n",
    "    ax1.grid(True_or_False)\n",
    "    \n",
    "    return 'two figures plot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_smooth(input_data):\n",
    "    \n",
    "    # make the data curve smooth    \n",
    "    \n",
    "    # define the differnece sequence\n",
    "    difference = input_data.diff().dropna()\n",
    "    \n",
    "    # define description of the sequence\n",
    "    information = difference.describe()\n",
    "    \n",
    "    # define maximum normal value\n",
    "    high_value = information['75%'] * 0.75\n",
    "    \n",
    "    # define minimum normal value\n",
    "    low_value = information['25%'] * 1.5\n",
    "    \n",
    "    # define index of abnormal values\n",
    "    abnormal_index = difference[(difference > high_value) | (difference < low_value)].index\n",
    "    \n",
    "    i = 0 \n",
    "    data = np.array(input_data)\n",
    "    while i < len(abnormal_index):\n",
    "        \n",
    "        # find how many continuous abnormal values\n",
    "        n = 1\n",
    "        \n",
    "        # the starting index of the abnormal values\n",
    "        start = abnormal_index[i]\n",
    "        if (i+n) < len(abnormal_index):   \n",
    "            \n",
    "            while abnormal_index[i+n] == (start + n):\n",
    "                n += 1\n",
    "                \n",
    "                if (i+n) >= len(abnormal_index):\n",
    "                    break                \n",
    "        i += n - 1\n",
    "        \n",
    "        # the ending index of the abnormal values\n",
    "        end = abnormal_index[i]\n",
    "        \n",
    "        # fill abnormal values with appropriate values\n",
    "        padding = input_data[start - 1]\n",
    "        value = np.linspace(padding, padding, n)\n",
    "        data[start:end+1] = value\n",
    "        i += 1\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(input_data):\n",
    "    \n",
    "    # normalize the input data\n",
    "    \n",
    "    # create normalization data list\n",
    "    new_data = []\n",
    "    \n",
    "    # define description of the input data\n",
    "    information = input_data.describe()\n",
    "    \n",
    "    # new_data = (old_data - old_data_min)/(old_data_max - old_data_min)\n",
    "    for i in input_data:\n",
    "         new_data.append((i - information['min']) / (information['max'] - information['min']))\n",
    "    \n",
    "    return np.array(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BP_Neural_Network:\n",
    "    \n",
    "    # back propagation nerual network algorithm\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # define the number of input/ hidden/ output neurons        \n",
    "        self.input_number = 0\n",
    "        self.hidden_number = 0\n",
    "        self.output_number = 0\n",
    "        \n",
    "        # define input/ hidden/ output data        \n",
    "        self.input_cells = []\n",
    "        self.hidden_cells = []\n",
    "        self.output_cells = []\n",
    "        \n",
    "        # define the initial input/ hidden weights\n",
    "        self.input_weights = []\n",
    "        self.hidden_weights = []\n",
    "        \n",
    "        # define the modified input/ hidden weights\n",
    "        self.input_modify = []\n",
    "        self.hidden_modify = []\n",
    "        \n",
    "                \n",
    "    def sigmoid(self, x):\n",
    "        \n",
    "        # sigmoid function\n",
    "        # 1 / (1 + e^(-x))\n",
    "        return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \n",
    "        # sigmoid derivative function\n",
    "        # sigmoid * (1 - sigmoid)\n",
    "        return x * (1 - x)\n",
    "\n",
    "\n",
    "    def rand_interval(self, start, end):\n",
    "        \n",
    "        # define random number\n",
    "        return (end - start) * random.random() + start\n",
    "\n",
    "\n",
    "    def weights_matrix(self, neuron_input, neuron_output, initial_fill=0.0):\n",
    "        \n",
    "        # define weights matrix\n",
    "        create_matrix = []\n",
    "        \n",
    "        for i in range(neuron_input):\n",
    "            create_matrix.append([initial_fill] * neuron_output)\n",
    "            \n",
    "        return create_matrix\n",
    "\n",
    "    \n",
    "    def initial_setup(self, input_number, hidden_number, output_number):\n",
    "        \n",
    "        # initial the number of input/ hidden/ output neurons\n",
    "        self.input_number = input_number + 1\n",
    "        self.hidden_number = hidden_number\n",
    "        self.output_number =  output_number\n",
    "        \n",
    "        # initial input/ hidden/ output cells\n",
    "        self.input_cells = [1.0] * self.input_number\n",
    "        self.hidden_cells = [1.0] * self.hidden_number\n",
    "        self.output_cells = [1.0] * self.output_number\n",
    "        \n",
    "        # random initial input weights\n",
    "        self.input_weights = self.weights_matrix(self.input_number, self.hidden_number)\n",
    "        \n",
    "        for i in range(self.input_number):            \n",
    "            for j in range(self.hidden_number):\n",
    "                self.input_weights[i][j] = self.rand_interval(-0.2, 0.2)\n",
    "                \n",
    "        # random initial hidden weights \n",
    "        self.hidden_weights = self.weights_matrix(self.hidden_number, self.output_number)\n",
    "        \n",
    "        for j in range(self.hidden_number):            \n",
    "            for k in range(self.output_number):\n",
    "                self.hidden_weights[j][k] = self.rand_interval(-0.2, 0.2)\n",
    "        \n",
    "        # initial modified input/ hidden weights\n",
    "        self.input_modify = self.weights_matrix(self.input_number, self.hidden_number)\n",
    "        self.hidden_modify = self.weights_matrix(self.hidden_number, self.output_number)\n",
    "        \n",
    "\n",
    "    def predict(self, input_data):\n",
    "        \n",
    "        # activate the input layer\n",
    "        for i in range(self.input_number - 1):\n",
    "            self.input_cells[i] = input_data[i]\n",
    "            \n",
    "        # activate the hidden layer    \n",
    "        for j in range(self.hidden_number):\n",
    "            sum_hidden = 0.0\n",
    "            \n",
    "            for i in range(self.input_number):\n",
    "                sum_hidden += self.input_cells[i] * self.input_weights[i][j]\n",
    "            self.hidden_cells[j] = self.sigmoid(sum_hidden)\n",
    "            \n",
    "        # activate the output layer\n",
    "        for k in range(self.output_number):\n",
    "            sum_output = 0.0\n",
    "            \n",
    "            for j in range(self.hidden_number):\n",
    "                sum_output += self.hidden_cells[j] * self.hidden_weights[j][k]\n",
    "            self.output_cells[k] = self.sigmoid(sum_output)\n",
    "            \n",
    "        return self.output_cells[:]\n",
    "    \n",
    "\n",
    "    def back_propagate(self, learning_sample, sample_label, learning_rate, momentum):\n",
    "        \n",
    "        # feed forward neural network\n",
    "        self.predict(learning_sample)\n",
    "        \n",
    "        # define output layer error\n",
    "        output_deltas = [0.0] * self.output_number\n",
    "        \n",
    "        for k in range(self.output_number):\n",
    "            output_error = sample_label[k] - self.output_cells[k]\n",
    "            output_deltas[k] = self.sigmoid_derivative(self.output_cells[k]) * output_error\n",
    "            \n",
    "        # define hidden layer error   \n",
    "        hidden_deltas = [0.0] * self.hidden_number\n",
    "        \n",
    "        for j in range(self.hidden_number):\n",
    "            output_error = 0.0\n",
    "            \n",
    "            for k in range(self.output_number):\n",
    "                output_error += output_deltas[k] * self.hidden_weights[j][k]\n",
    "            hidden_deltas[j] = self.sigmoid_derivative(self.hidden_cells[j]) * output_error\n",
    "            \n",
    "        # define modified hidden weights    \n",
    "        for j in range(self.hidden_number):\n",
    "            for k in range(self.output_number):\n",
    "                output_change = output_deltas[k] * self.hidden_cells[j]\n",
    "                self.hidden_weights[j][k] += learning_rate * output_change + momentum * self.hidden_modify[j][k]\n",
    "                self.hidden_modify[j][k] = output_change\n",
    "                \n",
    "        # define modified input weights   \n",
    "        for i in range(self.input_number):\n",
    "            for j in range(self.hidden_number):\n",
    "                hidden_change = hidden_deltas[j] * self.input_cells[i]\n",
    "                self.input_weights[i][j] += learning_rate * hidden_change + momentum * self.input_modify[i][j]\n",
    "                self.input_modify[i][j] = hidden_change\n",
    "                \n",
    "        # define global error\n",
    "        global_error = 0.0\n",
    "        for k in range(len(sample_label)):\n",
    "            global_error += 0.5 * (sample_label[k] - self.output_cells[k]) ** 2\n",
    "            \n",
    "        return global_error\n",
    "    \n",
    "\n",
    "    def training_set(self, learning_samples, sample_labels, maximum, learning_rate, momentum, error_convergence):\n",
    "        \n",
    "        # iterative cumulative error\n",
    "        for j in range(maximum):\n",
    "            global_error = 0.0  \n",
    "            \n",
    "            # calculate the global error of the training set\n",
    "            for i in range(len(learning_samples)):\n",
    "                sample_label = sample_labels[i]\n",
    "                learning_sample = learning_samples[i]\n",
    "                global_error += self.back_propagate(learning_sample, sample_label, learning_rate, momentum) \n",
    "            \n",
    "            # global error convergence judgment\n",
    "            if abs(global_error) <= error_convergence:\n",
    "                break\n",
    "        \n",
    "\n",
    "    def test_set(self, learning_samples, sample_labels, input_number, hidden_number, output_number, maximum, learning_rate, momentum, error_convergence):\n",
    "\n",
    "        # call initial_setup and training_set function\n",
    "        self.initial_setup(input_number, hidden_number, output_number)\n",
    "        self.training_set(learning_samples, sample_labels, maximum, learning_rate, momentum, error_convergence)\n",
    "        \n",
    "        # use learning samples for testing\n",
    "        for learning_sample in learning_samples:\n",
    "            self.predict(learning_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_simple_transform_csv(Elevation):\n",
    "\n",
    "    # transform list into csv\n",
    "    \n",
    "    # define name of sequences\n",
    "    column = Elevation\n",
    "    name =['Elevation']\n",
    "\n",
    "    # define csv\n",
    "    combine_list = []\n",
    "    \n",
    "    for i in range(len(column)):\n",
    "        combine_list.append([column[i]])\n",
    "    \n",
    "    data_csv = pd.DataFrame(columns = name, data = combine_list)\n",
    "\n",
    "    return data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_and_label(input_data, number, set_index, slice1, slice2, slice3, slice4, choice):\n",
    "    \n",
    "    # create training samples and sample labels\n",
    "    \n",
    "    # read normalized data and transform it into csv\n",
    "    data_nor = data_normalization(input_data)[:number]\n",
    "    csv = list_simple_transform_csv(data_nor)\n",
    "    \n",
    "    # define data list and csv index\n",
    "    data_list = data_nor.tolist()\n",
    "    index = csv.index.tolist()\n",
    "    \n",
    "    # define training samples\n",
    "    training_sample = []\n",
    "    \n",
    "    for i in index :\n",
    "        data_slice = data_list[i:i + slice1]\n",
    "        \n",
    "        if choice == 'true':\n",
    "            for j in range(set_index):\n",
    "                data_slice.append(1e-3)\n",
    "                \n",
    "        training_sample.append(data_slice)\n",
    "        \n",
    "        if i + slice2 == len(data_list) - 1:\n",
    "            break\n",
    "    \n",
    "    # define sample labels\n",
    "    sample_label = []\n",
    "    \n",
    "    for i in index:\n",
    "        sample_label.append([data_list[i + slice3]])\n",
    "        \n",
    "        if i + slice4 == len(data_list)-1:\n",
    "            break\n",
    "            \n",
    "    return training_sample, sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_historical_sample(input_data1, input_data2, number, index):\n",
    "    \n",
    "    # create input historical samples\n",
    "    \n",
    "    data_nor1 = data_normalization(input_data1)[:number]\n",
    "    data_nor2 = data_normalization(input_data2)[:number]\n",
    "    \n",
    "    new_data = (data_nor1 + data_nor2) / 2\n",
    "    \n",
    "    return new_data[:index].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_prediction_list(neural_network,historical_sample1, iterations, steps, number, choice):\n",
    "    \n",
    "    # use historical samples to do prediction\n",
    "    \n",
    "    for i in range(0, iterations):\n",
    "        historical_sample = historical_sample1\n",
    "        input_data = historical_sample[i:i+steps]\n",
    "        \n",
    "        if choice == 'true':\n",
    "            for i in range(number):\n",
    "                input_data.append(1e-3)\n",
    "                \n",
    "        prediction_data = neural_network.predict(input_data)\n",
    "        historical_sample.append(prediction_data[0])\n",
    "    \n",
    "    return historical_sample, len(historical_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transfer(input_data, normalized_data):\n",
    "    \n",
    "    # transfer normalized data into actual data\n",
    "    \n",
    "    # create actual data list\n",
    "    new_data = []\n",
    "    \n",
    "    # define description of the input data\n",
    "    information = input_data.describe()\n",
    "    \n",
    "    # new_data = (old_data * (old_data_max - old_data_min) + old_data_min)\n",
    "    for i in normalized_data:\n",
    "        new_data.append(i * (information['max'] - information['min']) + information['min'])\n",
    "    \n",
    "    return np.array(new_data), len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_coefficient(prediction, observation):\n",
    "    \n",
    "    # calculate correlation coefficient regarding prediction and observation\n",
    "    \n",
    "    # define mean of prediction and observation\n",
    "    prediction_csv = list_simple_transform_csv(prediction)\n",
    "    prediction_mean = prediction_csv['Elevation'].describe()['mean']\n",
    "    observation_mean = observation.describe()['mean']\n",
    "    \n",
    "    # define cov(x,y)\n",
    "    covariance_xy = 0\n",
    "    for i in range(0, len(prediction_csv)):\n",
    "        covariance_xy += (prediction_csv['Elevation'][i] - prediction_mean) * (observation[i] - observation_mean)\n",
    "    \n",
    "    # define var[x]\n",
    "    variance_x = 0\n",
    "    for i in range(0, len(prediction_csv)):\n",
    "        variance_x += (prediction_csv['Elevation'][i] - prediction_mean)**2\n",
    "    \n",
    "    # define var[y]\n",
    "    variance_y = 0\n",
    "    for i in range(0, len(prediction_csv)):\n",
    "        variance_y += (observation[i] - observation_mean)**2\n",
    "    \n",
    "    # r(x,y) = cov(x,y) / sqrt(var[x] * var[y])\n",
    "    correlation_coefficient = covariance_xy / np.sqrt(variance_x * variance_y)\n",
    "    \n",
    "    return correlation_coefficient"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
